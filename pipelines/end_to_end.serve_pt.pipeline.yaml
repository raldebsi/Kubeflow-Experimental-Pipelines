apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: end-to-end-hugging-face-topic-classifier-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.12, pipelines.kubeflow.org/pipeline_compilation_time: '2022-06-29T12:24:24.483757',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "End to End Topic Classiciation
      using HuggingFace Framework and CamelBert model", "inputs": [{"name": "experiment_name",
      "type": "String"}, {"name": "volume_name", "type": "String"}, {"name": "dataset_name",
      "type": "String"}], "name": "End to End Hugging Face Topic Classifier"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.12}
spec:
  entrypoint: end-to-end-hugging-face-topic-classifier
  templates:
  - name: end-to-end-hugging-face-topic-classifier
    inputs:
      parameters:
      - {name: dataset_name}
      - {name: experiment_name}
      - {name: volume_name}
    dag:
      tasks:
      - name: serve-a-model-with-kserve
        template: serve-a-model-with-kserve
        arguments:
          parameters:
          - {name: dataset_name, value: '{{inputs.parameters.dataset_name}}'}
          - {name: experiment_name, value: '{{inputs.parameters.experiment_name}}'}
          - {name: volume_name, value: '{{inputs.parameters.volume_name}}'}
  - name: serve-a-model-with-kserve
    container:
      args:
      - -u
      - kservedeployer.py
      - --action
      - apply
      - --model-name
      - ''
      - --model-uri
      - ''
      - --canary-traffic-percent
      - '100'
      - --namespace
      - ''
      - --framework
      - ''
      - --custom-model-spec
      - '{}'
      - --autoscaling-target
      - '0'
      - --service-account
      - ''
      - --enable-istio-sidecar
      - "True"
      - --output-path
      - /tmp/outputs/InferenceService_Status/data
      - --inferenceservice-yaml
      - |
        apiVersion: serving.kserve.io/v1beta1
        kind: InferenceService
        metadata:
          annotations:
            sidecar.istio.io/inject: 'false'
          name: '{{inputs.parameters.dataset_name}}_{{inputs.parameters.experiment_name}}'
          namespace: '{{workflow.namespace}}'
        spec:
          predictor:
            tensorflow:
              storageUri: pvc://{{inputs.parameters.volume_name}}/{{inputs.parameters.experiment_name}}/outputs/{{inputs.parameters.dataset_name}}
      - --watch-timeout
      - '300'
      - --min-replicas
      - '-1'
      - --max-replicas
      - '-1'
      - --request-timeout
      - '60'
      command: [python]
      image: quay.io/aipipeline/kserve-component:v0.7.0
      volumeMounts:
      - {mountPath: /store, name: volume-bind}
    inputs:
      parameters:
      - {name: dataset_name}
      - {name: experiment_name}
      - {name: volume_name}
    outputs:
      artifacts:
      - {name: serve-a-model-with-kserve-InferenceService-Status, path: /tmp/outputs/InferenceService_Status/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Serve
          Models using KServe", "implementation": {"container": {"args": ["-u", "kservedeployer.py",
          "--action", {"inputValue": "Action"}, "--model-name", {"inputValue": "Model
          Name"}, "--model-uri", {"inputValue": "Model URI"}, "--canary-traffic-percent",
          {"inputValue": "Canary Traffic Percent"}, "--namespace", {"inputValue":
          "Namespace"}, "--framework", {"inputValue": "Framework"}, "--custom-model-spec",
          {"inputValue": "Custom Model Spec"}, "--autoscaling-target", {"inputValue":
          "Autoscaling Target"}, "--service-account", {"inputValue": "Service Account"},
          "--enable-istio-sidecar", {"inputValue": "Enable Istio Sidecar"}, "--output-path",
          {"outputPath": "InferenceService Status"}, "--inferenceservice-yaml", {"inputValue":
          "InferenceService YAML"}, "--watch-timeout", {"inputValue": "Watch Timeout"},
          "--min-replicas", {"inputValue": "Min Replicas"}, "--max-replicas", {"inputValue":
          "Max Replicas"}, "--request-timeout", {"inputValue": "Request Timeout"}],
          "command": ["python"], "image": "quay.io/aipipeline/kserve-component:v0.7.0"}},
          "inputs": [{"default": "create", "description": "Action to execute on KServe",
          "name": "Action", "type": "String"}, {"default": "", "description": "Name
          to give to the deployed model", "name": "Model Name", "type": "String"},
          {"default": "", "description": "Path of the S3 or GCS compatible directory
          containing the model.", "name": "Model URI", "type": "String"}, {"default":
          "100", "description": "The traffic split percentage between the candidate
          model and the last ready model", "name": "Canary Traffic Percent", "type":
          "String"}, {"default": "", "description": "Kubernetes namespace where the
          KServe service is deployed.", "name": "Namespace", "type": "String"}, {"default":
          "", "description": "Machine Learning Framework for Model Serving.", "name":
          "Framework", "type": "String"}, {"default": "{}", "description": "Custom
          model runtime container spec in JSON", "name": "Custom Model Spec", "type":
          "String"}, {"default": "0", "description": "Autoscaling Target Number",
          "name": "Autoscaling Target", "type": "String"}, {"default": "", "description":
          "ServiceAccount to use to run the InferenceService pod", "name": "Service
          Account", "type": "String"}, {"default": "True", "description": "Whether
          to enable istio sidecar injection", "name": "Enable Istio Sidecar", "type":
          "Bool"}, {"default": "{}", "description": "Raw InferenceService serialized
          YAML for deployment", "name": "InferenceService YAML", "type": "String"},
          {"default": "300", "description": "Timeout seconds for watching until InferenceService
          becomes ready.", "name": "Watch Timeout", "type": "String"}, {"default":
          "-1", "description": "Minimum number of InferenceService replicas", "name":
          "Min Replicas", "type": "String"}, {"default": "-1", "description": "Maximum
          number of InferenceService replicas", "name": "Max Replicas", "type": "String"},
          {"default": "60", "description": "Specifies the number of seconds to wait
          before timing out a request to the component.", "name": "Request Timeout",
          "type": "String"}], "name": "Serve a model with KServe", "outputs": [{"description":
          "Status JSON output of InferenceService", "name": "InferenceService Status",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "cb3f9cdb6d51b05bddf7775e58a3b7351f9c923485fbc09a2c50eb6b281ba87a", "url":
          "src/pipelines/yamls/Components/kfserve_launcher.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"Action":
          "apply", "Autoscaling Target": "0", "Canary Traffic Percent": "100", "Custom
          Model Spec": "{}", "Enable Istio Sidecar": "True", "Framework": "", "InferenceService
          YAML": "apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  annotations:\n    sidecar.istio.io/inject:
          ''false''\n  name: ''{{inputs.parameters.dataset_name}}_{{inputs.parameters.experiment_name}}''\n  namespace:
          ''{{workflow.namespace}}''\nspec:\n  predictor:\n    tensorflow:\n      storageUri:
          pvc://{{inputs.parameters.volume_name}}/{{inputs.parameters.experiment_name}}/outputs/{{inputs.parameters.dataset_name}}\n",
          "Max Replicas": "-1", "Min Replicas": "-1", "Model Name": "", "Model URI":
          "", "Namespace": "", "Request Timeout": "60", "Service Account": "", "Watch
          Timeout": "300"}'}
    volumes:
    - name: volume-bind
      persistentVolumeClaim: {claimName: '{{inputs.parameters.volume_name}}'}
  arguments:
    parameters:
    - {name: experiment_name}
    - {name: volume_name}
    - {name: dataset_name}
  serviceAccountName: pipeline-runner
