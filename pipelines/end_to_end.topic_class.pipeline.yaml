apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: end-to-end-hugging-face-topic-classifier-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.12, pipelines.kubeflow.org/pipeline_compilation_time: '2022-06-29T17:22:13.509395',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "End to End Topic Classiciation
      using HuggingFace Framework and CamelBert model", "inputs": [{"name": "experiment_name",
      "type": "String"}, {"name": "volume_name", "type": "String"}, {"name": "dataset_name",
      "type": "String"}, {"default": "False", "name": "has_test", "optional": true,
      "type": "Boolean"}, {"default": "512", "name": "max_sequence_length", "optional":
      true, "type": "Integer"}, {"default": "8", "name": "device_batch_size", "optional":
      true, "type": "Integer"}, {"default": "3e-05", "name": "learning_rate", "optional":
      true, "type": "Float"}, {"default": "5", "name": "epochs", "optional": true,
      "type": "Integer"}, {"name": "seed", "optional": true, "type": "Integer"}],
      "name": "End to End Hugging Face Topic Classifier"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.12}
spec:
  entrypoint: end-to-end-hugging-face-topic-classifier
  templates:
  - name: convert-run-args-to-str
    container:
      args: [--train-args, '{{inputs.parameters.get-run-args-train_args}}', --extra-args,
        '{{inputs.parameters.get-run-args-extra_args}}', '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def convert_run_args_to_str(train_args, extra_args):
            print("Args")
            print(train_args)
            print("Extra Args")
            print(extra_args)
            e_args = " ".join("--{} {}".format(k, v) if v != "" else "--" + k for k, v in extra_args.items())
            p_args = " ".join("--{} {}".format(k, v) if v != "" else "--" + k for k, v in train_args.items())
            print("Formatted")
            print(p_args)
            print(e_args)

            return "{} {}".format(p_args, e_args)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Convert run args to str', description='')
        _parser.add_argument("--train-args", dest="train_args", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--extra-args", dest="extra_args", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = convert_run_args_to_str(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: python:3.7
    inputs:
      parameters:
      - {name: get-run-args-extra_args}
      - {name: get-run-args-train_args}
    outputs:
      parameters:
      - name: convert-run-args-to-str-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: convert-run-args-to-str-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--train-args", {"inputValue": "train_args"}, "--extra-args",
          {"inputValue": "extra_args"}, "----output-paths", {"outputPath": "Output"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def convert_run_args_to_str(train_args,
          extra_args):\n    print(\"Args\")\n    print(train_args)\n    print(\"Extra
          Args\")\n    print(extra_args)\n    e_args = \" \".join(\"--{} {}\".format(k,
          v) if v != \"\" else \"--\" + k for k, v in extra_args.items())\n    p_args
          = \" \".join(\"--{} {}\".format(k, v) if v != \"\" else \"--\" + k for k,
          v in train_args.items())\n    print(\"Formatted\")\n    print(p_args)\n    print(e_args)\n\n    return
          \"{} {}\".format(p_args, e_args)\n\ndef _serialize_str(str_value: str) ->
          str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(\n            str(str_value),
          str(type(str_value))))\n    return str_value\n\nimport json\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Convert run args to str'', description='''')\n_parser.add_argument(\"--train-args\",
          dest=\"train_args\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--extra-args\",
          dest=\"extra_args\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = convert_run_args_to_str(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "train_args", "type": "JsonObject"},
          {"name": "extra_args", "type": "JsonObject"}], "name": "Convert run args
          to str", "outputs": [{"name": "Output", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"extra_args": "{{inputs.parameters.get-run-args-extra_args}}",
          "train_args": "{{inputs.parameters.get-run-args-train_args}}"}'}
  - name: end-to-end-hugging-face-topic-classifier
    inputs:
      parameters:
      - {name: dataset_name}
      - {name: device_batch_size}
      - {name: epochs}
      - {name: experiment_name}
      - {name: has_test}
      - {name: learning_rate}
      - {name: max_sequence_length}
      - {name: seed}
      - {name: volume_name}
    dag:
      tasks:
      - name: convert-run-args-to-str
        template: convert-run-args-to-str
        dependencies: [get-run-args]
        arguments:
          parameters:
          - {name: get-run-args-extra_args, value: '{{tasks.get-run-args.outputs.parameters.get-run-args-extra_args}}'}
          - {name: get-run-args-train_args, value: '{{tasks.get-run-args.outputs.parameters.get-run-args-train_args}}'}
      - name: get-run-args
        template: get-run-args
        arguments:
          parameters:
          - {name: dataset_name, value: '{{inputs.parameters.dataset_name}}'}
          - {name: device_batch_size, value: '{{inputs.parameters.device_batch_size}}'}
          - {name: epochs, value: '{{inputs.parameters.epochs}}'}
          - {name: experiment_name, value: '{{inputs.parameters.experiment_name}}'}
          - {name: has_test, value: '{{inputs.parameters.has_test}}'}
          - {name: learning_rate, value: '{{inputs.parameters.learning_rate}}'}
          - {name: max_sequence_length, value: '{{inputs.parameters.max_sequence_length}}'}
          - {name: seed, value: '{{inputs.parameters.seed}}'}
          - {name: volume_name, value: '{{inputs.parameters.volume_name}}'}
      - name: huggingface-trainer
        template: huggingface-trainer
        dependencies: [convert-run-args-to-str]
        arguments:
          parameters:
          - {name: convert-run-args-to-str-Output, value: '{{tasks.convert-run-args-to-str.outputs.parameters.convert-run-args-to-str-Output}}'}
          - {name: volume_name, value: '{{inputs.parameters.volume_name}}'}
      - name: print-huggingface-trainer
        template: print-huggingface-trainer
        dependencies: [convert-run-args-to-str]
        arguments:
          parameters:
          - {name: convert-run-args-to-str-Output, value: '{{tasks.convert-run-args-to-str.outputs.parameters.convert-run-args-to-str-Output}}'}
      - name: serve-a-model-with-kserve
        template: serve-a-model-with-kserve
        dependencies: [huggingface-trainer]
        arguments:
          parameters:
          - {name: dataset_name, value: '{{inputs.parameters.dataset_name}}'}
          - {name: experiment_name, value: '{{inputs.parameters.experiment_name}}'}
          - {name: volume_name, value: '{{inputs.parameters.volume_name}}'}
  - name: get-run-args
    container:
      args: [--dataset-name, '{{inputs.parameters.dataset_name}}', --has-test, '{{inputs.parameters.has_test}}',
        --seq-len, '{{inputs.parameters.max_sequence_length}}', --batch-size-dev,
        '{{inputs.parameters.device_batch_size}}', --learn-rate, '{{inputs.parameters.learning_rate}}',
        --epochs, '{{inputs.parameters.epochs}}', --seed, '{{inputs.parameters.seed}}',
        --experiment-name, '{{inputs.parameters.experiment_name}}', '----output-paths',
        /tmp/outputs/train_args/data, /tmp/outputs/extra_args/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def get_run_args(dataset_name, has_test, seq_len, batch_size_dev, learn_rate, epochs, seed, experiment_name):
            import json
            import os
            args = {"extra": {}}

            args["save_steps"] = 1000
            args["extra"]["overwrite_output_dir"] = ""

            model_name_or_path = "CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth"
            args["model_name_or_path"] = model_name_or_path

            train_file = '/store/datasets/{}/train.json'.format(dataset_name)
            args["train_file"] = train_file
            args["extra"]["do_train"] = ""

            valid_file = '/store/datasets/{}/valid.json'.format(dataset_name)
            args["extra"]["validation_file"] = valid_file
            args["extra"]["do_eval"] = ""

            if has_test == True or (isinstance(has_test, str) and has_test.lower() in "true1yes"):
                test_file = '/store/datasets/{}/test.json'.format(dataset_name)
                args["extra"]["test_file"] = test_file
                args["extra"]["do_predict"] = ""

            args["max_seq_length"] = seq_len

            args["per_device_train_batch_size"] = batch_size_dev

            args["learning_rate"] = learn_rate

            args["num_train_epochs"] = epochs

            output_dir = '/store/{}/outputs/{}'.format(experiment_name, dataset_name)
            os.makedirs(output_dir, exist_ok=True)
            args["output_dir"] = output_dir

            if seed:
                args["extra"]["seed"] = hash(seed) # Hash of int is the same as int, hash of str is int

            # write the args to a file
            with open(os.path.join(output_dir, "{}-{}-best-hps.json".format(experiment_name, dataset_name)), "w") as f:
                json.dump(args, f)

            print("Args:")
            print(args)

            # convert args to string
            # return " ".join("--{} {}".format(k, v) for k, v in args.items())

            return args, args.pop("extra")

        def _serialize_json(obj) -> str:
            if isinstance(obj, str):
                return obj
            import json

            def default_serializer(obj):
                if hasattr(obj, 'to_struct'):
                    return obj.to_struct()
                else:
                    raise TypeError(
                        "Object of type '%s' is not JSON serializable and does not have .to_struct() method."
                        % obj.__class__.__name__)

            return json.dumps(obj, default=default_serializer, sort_keys=True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Get run args', description='')
        _parser.add_argument("--dataset-name", dest="dataset_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--has-test", dest="has_test", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--seq-len", dest="seq_len", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--batch-size-dev", dest="batch_size_dev", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--learn-rate", dest="learn_rate", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--epochs", dest="epochs", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--seed", dest="seed", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--experiment-name", dest="experiment_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = get_run_args(**_parsed_args)

        _output_serializers = [
            _serialize_json,
            _serialize_json,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: python:3.7
      volumeMounts:
      - {mountPath: /store, name: volume-bind}
    inputs:
      parameters:
      - {name: dataset_name}
      - {name: device_batch_size}
      - {name: epochs}
      - {name: experiment_name}
      - {name: has_test}
      - {name: learning_rate}
      - {name: max_sequence_length}
      - {name: seed}
      - {name: volume_name}
    outputs:
      parameters:
      - name: get-run-args-extra_args
        valueFrom: {path: /tmp/outputs/extra_args/data}
      - name: get-run-args-train_args
        valueFrom: {path: /tmp/outputs/train_args/data}
      artifacts:
      - {name: get-run-args-extra_args, path: /tmp/outputs/extra_args/data}
      - {name: get-run-args-train_args, path: /tmp/outputs/train_args/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--dataset-name", {"inputValue": "dataset_name"}, "--has-test",
          {"inputValue": "has_test"}, "--seq-len", {"inputValue": "seq_len"}, "--batch-size-dev",
          {"inputValue": "batch_size_dev"}, "--learn-rate", {"inputValue": "learn_rate"},
          "--epochs", {"inputValue": "epochs"}, "--seed", {"inputValue": "seed"},
          "--experiment-name", {"inputValue": "experiment_name"}, "----output-paths",
          {"outputPath": "train_args"}, {"outputPath": "extra_args"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def get_run_args(dataset_name, has_test,
          seq_len, batch_size_dev, learn_rate, epochs, seed, experiment_name):\n    import
          json\n    import os\n    args = {\"extra\": {}}\n\n    args[\"save_steps\"]
          = 1000\n    args[\"extra\"][\"overwrite_output_dir\"] = \"\"\n\n    model_name_or_path
          = \"CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth\"\n    args[\"model_name_or_path\"]
          = model_name_or_path\n\n    train_file = ''/store/datasets/{}/train.json''.format(dataset_name)\n    args[\"train_file\"]
          = train_file\n    args[\"extra\"][\"do_train\"] = \"\"\n\n    valid_file
          = ''/store/datasets/{}/valid.json''.format(dataset_name)\n    args[\"extra\"][\"validation_file\"]
          = valid_file\n    args[\"extra\"][\"do_eval\"] = \"\"\n\n    if has_test
          == True or (isinstance(has_test, str) and has_test.lower() in \"true1yes\"):\n        test_file
          = ''/store/datasets/{}/test.json''.format(dataset_name)\n        args[\"extra\"][\"test_file\"]
          = test_file\n        args[\"extra\"][\"do_predict\"] = \"\"\n\n    args[\"max_seq_length\"]
          = seq_len\n\n    args[\"per_device_train_batch_size\"] = batch_size_dev\n\n    args[\"learning_rate\"]
          = learn_rate\n\n    args[\"num_train_epochs\"] = epochs\n\n    output_dir
          = ''/store/{}/outputs/{}''.format(experiment_name, dataset_name)\n    os.makedirs(output_dir,
          exist_ok=True)\n    args[\"output_dir\"] = output_dir\n\n    if seed:\n        args[\"extra\"][\"seed\"]
          = hash(seed) # Hash of int is the same as int, hash of str is int\n\n    #
          write the args to a file\n    with open(os.path.join(output_dir, \"{}-{}-best-hps.json\".format(experiment_name,
          dataset_name)), \"w\") as f:\n        json.dump(args, f)\n\n    print(\"Args:\")\n    print(args)\n\n    #
          convert args to string\n    # return \" \".join(\"--{} {}\".format(k, v)
          for k, v in args.items())\n\n    return args, args.pop(\"extra\")\n\ndef
          _serialize_json(obj) -> str:\n    if isinstance(obj, str):\n        return
          obj\n    import json\n\n    def default_serializer(obj):\n        if hasattr(obj,
          ''to_struct''):\n            return obj.to_struct()\n        else:\n            raise
          TypeError(\n                \"Object of type ''%s'' is not JSON serializable
          and does not have .to_struct() method.\"\n                % obj.__class__.__name__)\n\n    return
          json.dumps(obj, default=default_serializer, sort_keys=True)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Get run args'', description='''')\n_parser.add_argument(\"--dataset-name\",
          dest=\"dataset_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--has-test\",
          dest=\"has_test\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--seq-len\",
          dest=\"seq_len\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size-dev\",
          dest=\"batch_size_dev\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--learn-rate\",
          dest=\"learn_rate\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--epochs\",
          dest=\"epochs\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--seed\",
          dest=\"seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--experiment-name\",
          dest=\"experiment_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = get_run_args(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_json,\n    _serialize_json,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "dataset_name"}, {"name": "has_test"},
          {"name": "seq_len"}, {"name": "batch_size_dev"}, {"name": "learn_rate"},
          {"name": "epochs"}, {"name": "seed"}, {"name": "experiment_name"}], "name":
          "Get run args", "outputs": [{"name": "train_args", "type": "JsonObject"},
          {"name": "extra_args", "type": "JsonObject"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"batch_size_dev": "{{inputs.parameters.device_batch_size}}",
          "dataset_name": "{{inputs.parameters.dataset_name}}", "epochs": "{{inputs.parameters.epochs}}",
          "experiment_name": "{{inputs.parameters.experiment_name}}", "has_test":
          "{{inputs.parameters.has_test}}", "learn_rate": "{{inputs.parameters.learning_rate}}",
          "seed": "{{inputs.parameters.seed}}", "seq_len": "{{inputs.parameters.max_sequence_length}}"}'}
    volumes:
    - name: volume-bind
      persistentVolumeClaim: {claimName: '{{inputs.parameters.volume_name}}'}
  - name: huggingface-trainer
    container:
      args: [python3 $0 $*, /workspace/train.py, '{{inputs.parameters.convert-run-args-to-str-Output}}']
      command: [sh, -c]
      image: abdullahsaal/hf_train@sha256:fb30378e91fba55f29e4cba84f74a90e12b6a02f1da1e39d4fc339c3bd79276c
      volumeMounts:
      - {mountPath: /store, name: volume-bind}
    inputs:
      parameters:
      - {name: convert-run-args-to-str-Output}
      - {name: volume_name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Launches
          a single task to train a HuggingFace model.", "implementation": {"container":
          {"args": ["python3 $0 $*", "/workspace/train.py", {"inputValue": "Params"}],
          "command": ["sh", "-c"], "image": "abdullahsaal/hf_train@sha256:fb30378e91fba55f29e4cba84f74a90e12b6a02f1da1e39d4fc339c3bd79276c"}},
          "inputs": [{"description": "The parameters for the training job.", "name":
          "Params", "type": "String"}], "name": "HuggingFace Trainer"}', pipelines.kubeflow.org/component_ref: '{"digest":
          "e6f07a0e463d73767622424c44eed5f4e7c3be636c37fa3c5027e7ab5b0b6c33", "url":
          "src/pipelines/yamls/Components/hf_trainer_internal.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"Params":
          "{{inputs.parameters.convert-run-args-to-str-Output}}"}'}
    volumes:
    - name: volume-bind
      persistentVolumeClaim: {claimName: '{{inputs.parameters.volume_name}}'}
  - name: print-huggingface-trainer
    container:
      args: [echo python3 $0 $*, /workspace/train.py, '{{inputs.parameters.convert-run-args-to-str-Output}}']
      command: [sh, -c]
      image: abdullahsaal/hf_train@sha256:fb30378e91fba55f29e4cba84f74a90e12b6a02f1da1e39d4fc339c3bd79276c
    inputs:
      parameters:
      - {name: convert-run-args-to-str-Output}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Prints
          the expected arguments and commands for HuggingFace Trainer", "implementation":
          {"container": {"args": ["echo python3 $0 $*", "/workspace/train.py", {"inputValue":
          "Params"}], "command": ["sh", "-c"], "image": "abdullahsaal/hf_train@sha256:fb30378e91fba55f29e4cba84f74a90e12b6a02f1da1e39d4fc339c3bd79276c"}},
          "inputs": [{"description": "The parameters for the training job.", "name":
          "Params", "type": "String"}], "name": "Print HuggingFace Trainer"}', pipelines.kubeflow.org/component_ref: '{"digest":
          "21848bd5758fca1d7c5f2d90146e14ee8c1cef2bfe605855ddfc3d939eaa4d59"}', pipelines.kubeflow.org/arguments.parameters: '{"Params":
          "{{inputs.parameters.convert-run-args-to-str-Output}}"}'}
  - name: serve-a-model-with-kserve
    container:
      args:
      - -u
      - kservedeployer.py
      - --action
      - apply
      - --model-name
      - ''
      - --model-uri
      - ''
      - --canary-traffic-percent
      - '100'
      - --namespace
      - ''
      - --framework
      - ''
      - --custom-model-spec
      - '{}'
      - --autoscaling-target
      - '0'
      - --service-account
      - ''
      - --enable-istio-sidecar
      - "True"
      - --output-path
      - /tmp/outputs/InferenceService_Status/data
      - --inferenceservice-yaml
      - |
        apiVersion: kserve3/beta4
        kind: InferenceService
        metadata:
          annotations:
            sidecar.istio.io/inject: 'false'
          name: '{{inputs.parameters.dataset_name}}_{{inputs.parameters.experiment_name}}'
          namespace: '{{workflow.namespace}}'
        spec:
          predictor:
            tensorflow:
              storageUri: pvc://{{inputs.parameters.volume_name}}/
      - --watch-timeout
      - '300'
      - --min-replicas
      - '-1'
      - --max-replicas
      - '-1'
      - --request-timeout
      - '60'
      command: [python]
      image: quay.io/aipipeline/kserve-component:v0.7.0
    inputs:
      parameters:
      - {name: dataset_name}
      - {name: experiment_name}
      - {name: volume_name}
    outputs:
      artifacts:
      - {name: serve-a-model-with-kserve-InferenceService-Status, path: /tmp/outputs/InferenceService_Status/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Serve
          Models using KServe", "implementation": {"container": {"args": ["-u", "kservedeployer.py",
          "--action", {"inputValue": "Action"}, "--model-name", {"inputValue": "Model
          Name"}, "--model-uri", {"inputValue": "Model URI"}, "--canary-traffic-percent",
          {"inputValue": "Canary Traffic Percent"}, "--namespace", {"inputValue":
          "Namespace"}, "--framework", {"inputValue": "Framework"}, "--custom-model-spec",
          {"inputValue": "Custom Model Spec"}, "--autoscaling-target", {"inputValue":
          "Autoscaling Target"}, "--service-account", {"inputValue": "Service Account"},
          "--enable-istio-sidecar", {"inputValue": "Enable Istio Sidecar"}, "--output-path",
          {"outputPath": "InferenceService Status"}, "--inferenceservice-yaml", {"inputValue":
          "InferenceService YAML"}, "--watch-timeout", {"inputValue": "Watch Timeout"},
          "--min-replicas", {"inputValue": "Min Replicas"}, "--max-replicas", {"inputValue":
          "Max Replicas"}, "--request-timeout", {"inputValue": "Request Timeout"}],
          "command": ["python"], "image": "quay.io/aipipeline/kserve-component:v0.7.0"}},
          "inputs": [{"default": "create", "description": "Action to execute on KServe",
          "name": "Action", "type": "String"}, {"default": "", "description": "Name
          to give to the deployed model", "name": "Model Name", "type": "String"},
          {"default": "", "description": "Path of the S3 or GCS compatible directory
          containing the model.", "name": "Model URI", "type": "String"}, {"default":
          "100", "description": "The traffic split percentage between the candidate
          model and the last ready model", "name": "Canary Traffic Percent", "type":
          "String"}, {"default": "", "description": "Kubernetes namespace where the
          KServe service is deployed.", "name": "Namespace", "type": "String"}, {"default":
          "", "description": "Machine Learning Framework for Model Serving.", "name":
          "Framework", "type": "String"}, {"default": "{}", "description": "Custom
          model runtime container spec in JSON", "name": "Custom Model Spec", "type":
          "String"}, {"default": "0", "description": "Autoscaling Target Number",
          "name": "Autoscaling Target", "type": "String"}, {"default": "", "description":
          "ServiceAccount to use to run the InferenceService pod", "name": "Service
          Account", "type": "String"}, {"default": "True", "description": "Whether
          to enable istio sidecar injection", "name": "Enable Istio Sidecar", "type":
          "Bool"}, {"default": "{}", "description": "Raw InferenceService serialized
          YAML for deployment", "name": "InferenceService YAML", "type": "String"},
          {"default": "300", "description": "Timeout seconds for watching until InferenceService
          becomes ready.", "name": "Watch Timeout", "type": "String"}, {"default":
          "-1", "description": "Minimum number of InferenceService replicas", "name":
          "Min Replicas", "type": "String"}, {"default": "-1", "description": "Maximum
          number of InferenceService replicas", "name": "Max Replicas", "type": "String"},
          {"default": "60", "description": "Specifies the number of seconds to wait
          before timing out a request to the component.", "name": "Request Timeout",
          "type": "String"}], "name": "Serve a model with KServe", "outputs": [{"description":
          "Status JSON output of InferenceService", "name": "InferenceService Status",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "cb3f9cdb6d51b05bddf7775e58a3b7351f9c923485fbc09a2c50eb6b281ba87a", "url":
          "src/pipelines/yamls/Components/kfserve_launcher.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"Action":
          "apply", "Autoscaling Target": "0", "Canary Traffic Percent": "100", "Custom
          Model Spec": "{}", "Enable Istio Sidecar": "True", "Framework": "", "InferenceService
          YAML": "apiVersion: kserve3/beta4\nkind: InferenceService\nmetadata:\n  annotations:\n    sidecar.istio.io/inject:
          ''false''\n  name: ''{{inputs.parameters.dataset_name}}_{{inputs.parameters.experiment_name}}''\n  namespace:
          ''{{workflow.namespace}}''\nspec:\n  predictor:\n    tensorflow:\n      storageUri:
          pvc://{{inputs.parameters.volume_name}}/\n", "Max Replicas": "-1", "Min
          Replicas": "-1", "Model Name": "", "Model URI": "", "Namespace": "", "Request
          Timeout": "60", "Service Account": "", "Watch Timeout": "300"}'}
  arguments:
    parameters:
    - {name: experiment_name}
    - {name: volume_name}
    - {name: dataset_name}
    - name: has_test
      value: "False"
    - {name: max_sequence_length, value: '512'}
    - {name: device_batch_size, value: '8'}
    - {name: learning_rate, value: 3e-05}
    - {name: epochs, value: '5'}
    - {name: seed}
  serviceAccountName: pipeline-runner
