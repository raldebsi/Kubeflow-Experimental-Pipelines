apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: end-to-end-hugging-face-topic-classifier-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.12, pipelines.kubeflow.org/pipeline_compilation_time: '2022-05-30T17:29:07.427496',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "End to End Topic Classiciation
      using HuggingFace Framework and CamelBert model", "inputs": [{"name": "experiment_name",
      "type": "String"}, {"name": "volume_name", "type": "String"}, {"name": "dataset_name",
      "type": "String"}, {"default": "False", "name": "has_valid", "optional": true,
      "type": "Boolean"}, {"default": "512", "name": "max_sequence_length", "optional":
      true, "type": "Integer"}, {"default": "8", "name": "device_batch_size", "optional":
      true, "type": "Integer"}, {"default": "3e-05", "name": "learning_rate", "optional":
      true, "type": "Float"}, {"default": "5", "name": "epochs", "optional": true,
      "type": "Integer"}, {"name": "seed", "optional": true, "type": "Integer"}],
      "name": "End to End Hugging Face Topic Classifier"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.12}
spec:
  entrypoint: end-to-end-hugging-face-topic-classifier
  templates:
  - name: create-ridhwan-volumes
    resource:
      action: create
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: new-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 4Gi
    outputs:
      parameters:
      - name: create-ridhwan-volumes-manifest
        valueFrom: {jsonPath: '{}'}
      - name: create-ridhwan-volumes-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: create-ridhwan-volumes-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: end-to-end-hugging-face-topic-classifier
    inputs:
      parameters:
      - {name: dataset_name}
      - {name: device_batch_size}
      - {name: epochs}
      - {name: experiment_name}
      - {name: has_valid}
      - {name: learning_rate}
      - {name: max_sequence_length}
      - {name: seed}
    dag:
      tasks:
      - {name: create-ridhwan-volumes, template: create-ridhwan-volumes}
      - name: get-run-args
        template: get-run-args
        dependencies: [create-ridhwan-volumes]
        arguments:
          parameters:
          - {name: create-ridhwan-volumes-name, value: '{{tasks.create-ridhwan-volumes.outputs.parameters.create-ridhwan-volumes-name}}'}
          - {name: dataset_name, value: '{{inputs.parameters.dataset_name}}'}
          - {name: device_batch_size, value: '{{inputs.parameters.device_batch_size}}'}
          - {name: epochs, value: '{{inputs.parameters.epochs}}'}
          - {name: experiment_name, value: '{{inputs.parameters.experiment_name}}'}
          - {name: has_valid, value: '{{inputs.parameters.has_valid}}'}
          - {name: learning_rate, value: '{{inputs.parameters.learning_rate}}'}
          - {name: max_sequence_length, value: '{{inputs.parameters.max_sequence_length}}'}
          - {name: seed, value: '{{inputs.parameters.seed}}'}
      - name: kubeflow-launch-tfjob
        template: kubeflow-launch-tfjob
        dependencies: [create-ridhwan-volumes, get-run-args]
        arguments:
          parameters:
          - {name: create-ridhwan-volumes-name, value: '{{tasks.create-ridhwan-volumes.outputs.parameters.create-ridhwan-volumes-name}}'}
          - {name: experiment_name, value: '{{inputs.parameters.experiment_name}}'}
          - {name: get-run-args-Output, value: '{{tasks.get-run-args.outputs.parameters.get-run-args-Output}}'}
  - name: get-run-args
    container:
      args: [--dataset-name, '{{inputs.parameters.dataset_name}}', --has-valid, '{{inputs.parameters.has_valid}}',
        --seq-len, '{{inputs.parameters.max_sequence_length}}', --batch-size-dev,
        '{{inputs.parameters.device_batch_size}}', --learn-rate, '{{inputs.parameters.learning_rate}}',
        --epochs, '{{inputs.parameters.epochs}}', --seed, '{{inputs.parameters.seed}}',
        --experiment-name, '{{inputs.parameters.experiment_name}}', '----output-paths',
        /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def get_run_args(dataset_name, has_valid, seq_len, batch_size_dev, learn_rate, epochs, seed, experiment_name):
            import os
            import json
            args = {}

            model_name_or_path = "CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth"
            args["model_name_or_path"] = model_name_or_path

            train_file = '/store/datasets/{}/train.json'.format(dataset_name)
            args["train-file"] = train_file
            # args.append("--train_file={}".format(train_file))

            if has_valid:
                valid_file = '/store/datasets/{}/valid.json'.format(dataset_name)
                args["valid-file"] = valid_file
                # args.append("--valid_file={}".format(valid_file))

            args["max_seq_length-len"] = seq_len
            # args.append("--max_seq_length={}".format(seq_len))

            args["per_device_train_batch_size"] = batch_size_dev
            # args.append("--per_device_train_batch_size={}".format(batch_size_dev))

            args["learning_rate"] = learn_rate
            # args.append("--learning_rate={}".format(learn_rate))

            args["num_train_epochs"] = epochs
            # args.append("--num_train_epochs={}".format(epochs))

            output_dir = '/store/{}/outputs/{}'.format(experiment_name, dataset_name)
            os.makedirs(output_dir, exist_ok=True)
            args["output_dir"] = output_dir
            # args.append("--output_dir={}".format(output_dir))

            try:
                seed = int(seed)
                args["seed"] = seed
                # args.append("--seed={}".format(seed))
            except ValueError:
                pass

            # write the args to a file
            with open(os.path.join(output_dir, "{}-{}-best-hps.json".format(experiment_name, dataset_name)), "w") as f:
                json.dump(args, f)

            # convert args to string
            return " ".join("--{}={}".format(k, v) for k, v in args.items())

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Get run args', description='')
        _parser.add_argument("--dataset-name", dest="dataset_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--has-valid", dest="has_valid", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--seq-len", dest="seq_len", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--batch-size-dev", dest="batch_size_dev", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--learn-rate", dest="learn_rate", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--epochs", dest="epochs", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--seed", dest="seed", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--experiment-name", dest="experiment_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = get_run_args(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: python:3.7
      volumeMounts:
      - {mountPath: /store, name: create-ridhwan-volumes}
    inputs:
      parameters:
      - {name: create-ridhwan-volumes-name}
      - {name: dataset_name}
      - {name: device_batch_size}
      - {name: epochs}
      - {name: experiment_name}
      - {name: has_valid}
      - {name: learning_rate}
      - {name: max_sequence_length}
      - {name: seed}
    outputs:
      parameters:
      - name: get-run-args-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: get-run-args-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--dataset-name", {"inputValue": "dataset_name"}, "--has-valid",
          {"inputValue": "has_valid"}, "--seq-len", {"inputValue": "seq_len"}, "--batch-size-dev",
          {"inputValue": "batch_size_dev"}, "--learn-rate", {"inputValue": "learn_rate"},
          "--epochs", {"inputValue": "epochs"}, "--seed", {"inputValue": "seed"},
          "--experiment-name", {"inputValue": "experiment_name"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def get_run_args(dataset_name, has_valid, seq_len, batch_size_dev, learn_rate,
          epochs, seed, experiment_name):\n    import os\n    import json\n    args
          = {}\n\n    model_name_or_path = \"CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth\"\n    args[\"model_name_or_path\"]
          = model_name_or_path\n\n    train_file = ''/store/datasets/{}/train.json''.format(dataset_name)\n    args[\"train-file\"]
          = train_file\n    # args.append(\"--train_file={}\".format(train_file))\n\n    if
          has_valid:\n        valid_file = ''/store/datasets/{}/valid.json''.format(dataset_name)\n        args[\"valid-file\"]
          = valid_file\n        # args.append(\"--valid_file={}\".format(valid_file))\n\n    args[\"max_seq_length-len\"]
          = seq_len\n    # args.append(\"--max_seq_length={}\".format(seq_len))\n\n    args[\"per_device_train_batch_size\"]
          = batch_size_dev\n    # args.append(\"--per_device_train_batch_size={}\".format(batch_size_dev))\n\n    args[\"learning_rate\"]
          = learn_rate\n    # args.append(\"--learning_rate={}\".format(learn_rate))\n\n    args[\"num_train_epochs\"]
          = epochs\n    # args.append(\"--num_train_epochs={}\".format(epochs))\n\n    output_dir
          = ''/store/{}/outputs/{}''.format(experiment_name, dataset_name)\n    os.makedirs(output_dir,
          exist_ok=True)\n    args[\"output_dir\"] = output_dir\n    # args.append(\"--output_dir={}\".format(output_dir))\n\n    try:\n        seed
          = int(seed)\n        args[\"seed\"] = seed\n        # args.append(\"--seed={}\".format(seed))\n    except
          ValueError:\n        pass\n\n    # write the args to a file\n    with open(os.path.join(output_dir,
          \"{}-{}-best-hps.json\".format(experiment_name, dataset_name)), \"w\") as
          f:\n        json.dump(args, f)\n\n    # convert args to string\n    return
          \" \".join(\"--{}={}\".format(k, v) for k, v in args.items())\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(\n            str(str_value),
          str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Get run args'', description='''')\n_parser.add_argument(\"--dataset-name\",
          dest=\"dataset_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--has-valid\",
          dest=\"has_valid\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--seq-len\",
          dest=\"seq_len\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size-dev\",
          dest=\"batch_size_dev\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--learn-rate\",
          dest=\"learn_rate\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--epochs\",
          dest=\"epochs\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--seed\",
          dest=\"seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--experiment-name\",
          dest=\"experiment_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = get_run_args(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "dataset_name"}, {"name": "has_valid"},
          {"name": "seq_len"}, {"name": "batch_size_dev"}, {"name": "learn_rate"},
          {"name": "epochs"}, {"name": "seed"}, {"name": "experiment_name"}], "name":
          "Get run args", "outputs": [{"name": "Output", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"batch_size_dev": "{{inputs.parameters.device_batch_size}}",
          "dataset_name": "{{inputs.parameters.dataset_name}}", "epochs": "{{inputs.parameters.epochs}}",
          "experiment_name": "{{inputs.parameters.experiment_name}}", "has_valid":
          "{{inputs.parameters.has_valid}}", "learn_rate": "{{inputs.parameters.learning_rate}}",
          "seed": "{{inputs.parameters.seed}}", "seq_len": "{{inputs.parameters.max_sequence_length}}"}'}
    volumes:
    - name: create-ridhwan-volumes
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-ridhwan-volumes-name}}'}
  - name: kubeflow-launch-tfjob
    container:
      args:
      - --name
      - '{{inputs.parameters.experiment_name}}-{{workflow.name}}'
      - --namespace
      - '{{workflow.namespace}}'
      - --version
      - v1
      - --activeDeadlineSeconds
      - '-1'
      - --backoffLimit
      - '-1'
      - --cleanPodPolicy
      - Running
      - --ttlSecondsAfterFinished
      - '-1'
      - --psSpec
      - '{}'
      - --workerSpec
      - '{"replicas": 1, "restartPolicy": "OnFailure", "template": {"metadata": {"annotations":
        {"sidecar.istio.io/inject": "false"}}, "spec": {"containers": [{"name": "huggingface",
        "image": "abdullahsaal/hf_train@sha256:fb30378e91fba55f29e4cba84f74a90e12b6a02f1da1e39d4fc339c3bd79276c",
        "command": ["sh", "-c"], "args": ["python3 /workspace/train.py {{inputs.parameters.get-run-args-Output}}"]}]}}}'
      - --chiefSpec
      - '{"replicas": 1, "restartPolicy": "OnFailure", "template": {"metadata": {"annotations":
        {"sidecar.istio.io/inject": "false"}}, "spec": {"containers": [{"name": "huggingface",
        "image": "abdullahsaal/hf_train@sha256:fb30378e91fba55f29e4cba84f74a90e12b6a02f1da1e39d4fc339c3bd79276c",
        "command": ["sh", "-c"], "args": ["python3 /workspace/train.py {{inputs.parameters.get-run-args-Output}}"]}]}}}'
      - --evaluatorSpec
      - '{}'
      - --tfjobTimeoutMinutes
      - '60'
      - --deleteAfterDone
      - "False"
      command: [python, /ml/launch_tfjob.py]
      image: nikenano/launchernew:latest
      volumeMounts:
      - {mountPath: /store, name: create-ridhwan-volumes}
    inputs:
      parameters:
      - {name: create-ridhwan-volumes-name}
      - {name: experiment_name}
      - {name: get-run-args-Output}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Kubeflow
          TFJob launcher", "implementation": {"container": {"args": ["--name", {"inputValue":
          "Name"}, "--namespace", {"inputValue": "Namespace"}, "--version", {"inputValue":
          "Version"}, "--activeDeadlineSeconds", {"inputValue": "ActiveDeadlineSeconds"},
          "--backoffLimit", {"inputValue": "BackoffLimit"}, "--cleanPodPolicy", {"inputValue":
          "CleanPodPolicy"}, "--ttlSecondsAfterFinished", {"inputValue": "ttl Seconds
          After Finished"}, "--psSpec", {"inputValue": "PS Spec"}, "--workerSpec",
          {"inputValue": "Worker Spec"}, "--chiefSpec", {"inputValue": "Chief Spec"},
          "--evaluatorSpec", {"inputValue": "Evaluator Spec"}, "--tfjobTimeoutMinutes",
          {"inputValue": "Tfjob Timeout Minutes"}, "--deleteAfterDone", {"inputValue":
          "Delete Finished Tfjob"}], "command": ["python", "/ml/launch_tfjob.py"],
          "image": "nikenano/launchernew:latest"}}, "inputs": [{"description": "TFJob
          name.", "name": "Name", "type": "String"}, {"default": "kubeflow", "description":
          "TFJob namespace.", "name": "Namespace", "type": "String"}, {"default":
          "v1", "description": "TFJob version.", "name": "Version", "type": "String"},
          {"default": -1, "description": "Specifies the duration (in seconds) since
          startTime during which the job can remain active before it is terminated.
          Must be a positive integer. This setting applies only to pods where restartPolicy
          is OnFailure or Always.", "name": "ActiveDeadlineSeconds", "type": "Integer"},
          {"default": -1, "description": "Number of retries before marking this job
          as failed.", "name": "BackoffLimit", "type": "Integer"}, {"default": -1,
          "description": "Defines the TTL for cleaning up finished TFJobs.", "name":
          "ttl Seconds After Finished", "type": "Integer"}, {"default": "Running",
          "description": "Defines the policy for cleaning up pods after the TFJob
          completes.", "name": "CleanPodPolicy", "type": "String"}, {"default": "{}",
          "description": "TFJob ps replicaSpecs.", "name": "PS Spec", "type": "JsonObject"},
          {"default": "{}", "description": "TFJob worker replicaSpecs.", "name": "Worker
          Spec", "type": "JsonObject"}, {"default": "{}", "description": "TFJob chief
          replicaSpecs.", "name": "Chief Spec", "type": "JsonObject"}, {"default":
          "{}", "description": "TFJob evaluator replicaSpecs.", "name": "Evaluator
          Spec", "type": "JsonObject"}, {"default": 1440, "description": "Time in
          minutes to wait for the TFJob to complete.", "name": "Tfjob Timeout Minutes",
          "type": "Integer"}, {"default": "True", "description": "Whether to delete
          the tfjob after it is finished.", "name": "Delete Finished Tfjob", "type":
          "Bool"}], "name": "Kubeflow - Launch TFJob"}', pipelines.kubeflow.org/component_ref: '{"digest":
          "e529c8c347d7f8f8823cdce6b28707e96e8294bc2589f83b3aba7542db27df77", "url":
          "src/pipelines/yamls/Components/tfjob_launcher.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"ActiveDeadlineSeconds":
          "-1", "BackoffLimit": "-1", "Chief Spec": "{\"replicas\": 1, \"restartPolicy\":
          \"OnFailure\", \"template\": {\"metadata\": {\"annotations\": {\"sidecar.istio.io/inject\":
          \"false\"}}, \"spec\": {\"containers\": [{\"name\": \"huggingface\", \"image\":
          \"abdullahsaal/hf_train@sha256:fb30378e91fba55f29e4cba84f74a90e12b6a02f1da1e39d4fc339c3bd79276c\",
          \"command\": [\"sh\", \"-c\"], \"args\": [\"python3 /workspace/train.py
          {{inputs.parameters.get-run-args-Output}}\"]}]}}}", "CleanPodPolicy": "Running",
          "Delete Finished Tfjob": "False", "Evaluator Spec": "{}", "Name": "{{inputs.parameters.experiment_name}}-{{workflow.name}}",
          "Namespace": "{{workflow.namespace}}", "PS Spec": "{}", "Tfjob Timeout Minutes":
          "60", "Version": "v1", "Worker Spec": "{\"replicas\": 1, \"restartPolicy\":
          \"OnFailure\", \"template\": {\"metadata\": {\"annotations\": {\"sidecar.istio.io/inject\":
          \"false\"}}, \"spec\": {\"containers\": [{\"name\": \"huggingface\", \"image\":
          \"abdullahsaal/hf_train@sha256:fb30378e91fba55f29e4cba84f74a90e12b6a02f1da1e39d4fc339c3bd79276c\",
          \"command\": [\"sh\", \"-c\"], \"args\": [\"python3 /workspace/train.py
          {{inputs.parameters.get-run-args-Output}}\"]}]}}}", "ttl Seconds After Finished":
          "-1"}'}
    volumes:
    - name: create-ridhwan-volumes
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-ridhwan-volumes-name}}'}
  arguments:
    parameters:
    - {name: experiment_name}
    - {name: volume_name}
    - {name: dataset_name}
    - name: has_valid
      value: "False"
    - {name: max_sequence_length, value: '512'}
    - {name: device_batch_size, value: '8'}
    - {name: learning_rate, value: 3e-05}
    - {name: epochs, value: '5'}
    - {name: seed}
  serviceAccountName: pipeline-runner
